<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yujiao Shi</title>
  
  <meta name="author" content="Yujiao Shi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yujiao Shi</name>
              </p>
              <p>I am a PhD student at <a href="https://www.anu.edu.au">Australian National University (ANU)</a>, supervised by <a href="http://users.cecs.anu.edu.au/~hongdong/">Prof. Hongdong Li</a>. My research topics include satellite image-based localization, cross-view synthesis, 3D reconstruction.
              </p>
              <p>I was a research intern at Meta Reality Lab Researchs (RLR) from Jan 2022 to Jun 2022, working with Dr. Ziyun Li on burst image super-resolution and on-sensor computing. 
              </p>
	      </p>
              <p>I am now a research intern at Tencent Lab.
	      </p>
              <p style="text-align:center">
                <a href="mailto:yujiao.shi@anu.edu.au">Email</a> &nbsp/&nbsp
                <a href="data/CV_Yujiao_Shi.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=rVsRpZEAAAAJ&view_op=list_works&gmla=AJsN-F4i4kUg3TtyNRZTp39tnZS_1sJkviqHJbsrWQWvrjV6qxJzqssItls0txWgYlEdtuQ5FIl6PVjvG-y4uGO8Xn1Kn1gx8m-eJgOdamz8EAHBED_eOvI">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/YujiaoShi">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/shiyujiao">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yujiao-shi-053a12198">Linkedin</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Yujiao%20Shi.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/YujiaoShiCircle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2022: I start an internship at Tencent</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2022: One paper accepted to TPAMI 2022 </li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2022: One paper accepted to CVPR 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2022: I start an internship at Meta Reality Lab Research</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">12/2021: One paper accepted to TPAMI 2022</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2021: I am nominated as one of the CVPR 2021 outstanding reviewer</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2021: One paper accepted to CVPR 2021</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2020: One paper accepted to CVPR 2020</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2019: One paper accepted to AAAI 2020</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2019: One paper accepted to NeurIPS 2019</li>

                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/HighlyAccurate.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_Beyond_Cross-View_Image_Retrieval_Highly_Accurate_Vehicle_Localization_Using_Satellite_CVPR_2022_paper.pdf">
                <papertitle>Beyond Cross-view Image Retrieval: Highly Accurate Vehicle Localization Using Satellite Image </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Hongdong Li
              <br>
              CVPR, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_Beyond_Cross-View_Image_Retrieval_Highly_Accurate_Vehicle_Localization_Using_Satellite_CVPR_2022_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/HighlyAccurate"> code </a>
              <p></p>
              <p> We introduce a new pose optimization method to accurately pinpoint which pixel in a satellite image corresponds to the query camera location.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/3DOF.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2203.14148.pdf">
                <papertitle>Accurate 3-DoF Camera Geo-Localization via Ground-to-Satellite Image Matching </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Liu Liu, Dylan Campbell, Piotr Koniusz, and Hongdong Li
              <br>
              TPAMI, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.14148.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/IBL"> code </a>
              <p></p>
              <p> We propose projective transform, which (1) compliments polar transform to achieve better coarse localization performance and (2) provides a novel handcrafted method to accurately localize query camera on its matching satellite image.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Sat2Str.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2103.01623.pdf">
                <papertitle>Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Dylan Campbell, Xin Yu, and Hongdong Li
              <br>
              TPAMI, 2022
              <br>
              <a href="https://arxiv.org/pdf/2103.01623.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Sat2StrPanoramaSynthesis"> code </a>
              <p></p>
              <p> Satellite to street-view panorama synthesis, implicit satellite image height map estimation.
              </p>
            </td>
          </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SVNVS.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Self-Supervised_Visibility_Learning _for_Novel_View_Synthesis_CVPR_2021_paper.pdf">
                <papertitle>Self-Supervised Visibility Learning for Novel View Synthesis </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Hongdong Li, and Xin Yu
              <br>
              CVPR, 2021
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Self-Supervised_Visibility_Learning_for_Novel_View_Synthesis_CVPR_2021_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/SVNVS"> code </a>
              <p></p>
              <p> We estimate target-view depth and source-view visibility in an end-to-end manner.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/DSM.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Where_Am_I_Looking_At_Joint_Location_and_Orientation_Estimation_CVPR_2020_paper.pdf">
                <papertitle>Where am I looking at? Joint Location and Orientation Estimation by Cross-View Matching </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Dylan Campbell, and Hongdong Li
              <br>
              CVPR, 2020
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Where_Am_I_Looking_At_Joint_Location_and_Orientation_Estimation_CVPR_2020_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_DSM"> code </a>
              <p></p>
              <p> The first 3-DoF camera pose estimation framework via ground-to-satellite image matching.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVFT.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6875">
                <papertitle>Optimal Feature Transport for Cross-View Image Geo-Localization </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Liu Liu, Tong Zhang, and Hongdong Li
              <br>
              AAAI, 2020
              <br>
              <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6875"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_CVFT"> code </a>
              <p></p>
              <p> Motivated by optimal transport, we invent a cross-view feature transport module to bridge the cross-view domain gap.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SAFA.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf">
                <papertitle>Spatial-Aware Feature Aggregation for Cross-View Image based Geo-Localization </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Liu Liu, Xin Yu, and Hongdong Li
              <br>
              NeurIPS, 2019
              <br>
              <a href="https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_SAFA"> code </a>
              <p></p>
              <p> A polar transform is introduced to bridge the ground-and-satellite domain gap, which significantly boosts the state-of-the-art cross-view localization performance.
              </p>
            </td>
          </tr>


        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Service</heading>
              <p> Conference Program Committee Member/Reviewer:
              </p>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Conference on Computer Vision (ICCV), 2021</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">European Conference on Computer Vision (ECCV), 2020, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Asian Conference on Computer Vision (ACCV), 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Neural Information Processing Systems (NeurIPS), 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">AAAI Conference on Artificial Intelligence (AAAI), 2021</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Joint Conference on Artificial Intelligence (IJCAI), 2020, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Conference on 3D Vision (3DV), 2022</li>

                </ul>
              </p>
              <p> Journal Reviewer:
              </p>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Journal of Computer Vision (IJCV)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Image Processing (TIP)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The IEEE Robotics and Automation Letters (RAL)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Journal of Digital Earth</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Artificial Intelligence</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Knowledge-Based Systems</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ISPRS Journal of Photogrammetry and Remote Sensing</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        

					
        </tbody></table>
        <p align="center">
          <font size="2">
            Template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
          </font>
        </p>
      </td>
    </tr>
  </table>
</body>

</html>
