<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yujiao Shi</title>
  
  <meta name="author" content="Yujiao Shi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yujiao Shi</name>
              </p>
              <p>
		      I am an assistant professor at ShanghaiTech University. I was a research fellow and completed my Ph.D. at the Australian National University, supervised by Prof. Hongdong Li. My research focuses on camera localization, 3D reconstruction, view synthesis, and scene generation, particularly from aerial perspectives.
<!-- 		      I am an Assistant Professor at ShanghaiTech University. Previously, I was a Postdoc and PhD student at <a href="https://www.anu.edu.au">Australian National University (ANU)</a>, supervised by <a href="http://users.cecs.anu.edu.au/~hongdong/">Prof. Hongdong Li</a>. My research topics include satellite image-based localization, cross-view synthesis, 3D reconstruction. -->
              </p>
<!--               <p>I was a research intern at Meta Reality Lab Research (RLR) from Jan 2022 to Jun 2022, working with Dr. Ziyun Li on burst image super-resolution and on-sensor computing. I also did an internship at Tencent XR Lab from Jul to Oct 2022. 
              </p> -->
              <p style="text-align:center">
                <a href="mailto:shiyj2@shanghaitech.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/CV_YujiaoShi.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=rVsRpZEAAAAJ&view_op=list_works&gmla=AJsN-F4i4kUg3TtyNRZTp39tnZS_1sJkviqHJbsrWQWvrjV6qxJzqssItls0txWgYlEdtuQ5FIl6PVjvG-y4uGO8Xn1Kn1gx8m-eJgOdamz8EAHBED_eOvI">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/YujiaoShi">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/yujiaoshi">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yujiao-shi-053a12198">Linkedin</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Yujiao%20Shi.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/YujiaoShiCircle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Potential Students:</heading>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Looking for highly <strong>self-motivated undergraduate/master students</strong> with strong learning and problem-solving abilities working on challenging computer vision research problems! </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><strong>Postdoc, visiting undergraduate/master/PhD students, and RA positions</strong> are also available. </li>
		   
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
<!--                     <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Postdoc and visiting positions available, please drop me an email if you are interested. </li> -->
		<li style="list-style-position: inside;margin:0;padding:0;text-align: left;">04/2025: We will organize a <span style=" color: blue;"> workshop at ICCV: "From Street to Space: 3D Vision Across Altitudes"</span>. Check our workshop webpage <a href="https://3d-vast.github.io/"> here </a>.</li>    
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2025: I will serve as an Area Chair for NeurIPS 2025 </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2025: One paper is accepted at ICLR 2025, congrats Xianghui! </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2024: Two papers are accepted at 3DV 2025 </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2024: One paper is accepted at Siggraph Asia 2024 </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2024: Two papers are accepted at ECCV 2024 </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2024: I am serving as an Area Chair for NeurIPS 2024 </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2024: Our workshop proposal "UAVs in Multimedia" is accepted at ACM MM 2024 </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2024: I am serving as a co-organizer for Women in Computer Vision Workshop at ECCV 2024 </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2024: One paper accepted to ICRA 2024 </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2023: Co-organizer of workshop "UAVs in Multimedia" at ACM MM 2023</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2023: One paper accepted to NeurIPS 2023 </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2023: One paper accepted to IROS 2023 </li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2023: Tutorial speaker on cross-model camera localization at CVPR 2023 </li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2022: I start an internship at Tencent</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2022: One paper accepted to TPAMI 2022 </li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2022: One paper accepted to CVPR 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2022: I start an internship at Meta Reality Lab Research</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">12/2021: One paper accepted to TPAMI 2022</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2021: I am nominated as one of the CVPR 2021 outstanding reviewer</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2021: One paper accepted to CVPR 2021</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2020: One paper accepted to CVPR 2020</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2019: One paper accepted to AAAI 2020</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2019: One paper accepted to NeurIPS 2019</li>

                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table> -->



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/BevSplat.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a>
                <papertitle>BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization </papertitle>
              </a>
              <br>
              Qiwei Wang, Shaoxun Wu, and <strong>Yujiao Shi * </strong>
              <br>
              2025
              <br>
<!--               <a href="https://arxiv.org/pdf/2307.08015.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Boosting3DoFAccuracy"> code </a>
              <p></p>
              <p> This work proposes a decoupled rotation and translation estimation method for cross-view image matching, achieving significant performance improvement. 
              </p> -->
	      <p></p>
            </td>
        </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/AerialGo.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a>
                <papertitle>AerialGo: Walking-through City View Generation from Aerial Perspectives </papertitle>
              </a>
              <br>
              Fuqiang Zhao, Yijing Guo, Siyuan Yang, Xi Chen, Luo Wang, Lan Xu, Yingliang Zhang, <strong>Yujiao Shi</strong> *, and Jingyi Yu *
              <br>
              2025
              <br>
<!--               <a href="https://arxiv.org/pdf/2307.08015.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Boosting3DoFAccuracy"> code </a>
              <p></p>
              <p> This work proposes a decoupled rotation and translation estimation method for cross-view image matching, achieving significant performance improvement. 
              </p> -->
	      <p></p>
            </td>
        </tr>
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/ICLR25.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2502.03498">
                <papertitle>Controllable Satellite-to-Street-View Synthesis with Precise Pose Alignment and Zero-Shot Environmental Control </papertitle>
              </a>
              <br>
              Xianghui Ze, Zhenbo Song, Qiwei Wang, Jianfeng Lu, and <strong>Yujiao Shi * </strong>
              <br>
              ICLR, 2025
              <br>
<!--               <a href="https://arxiv.org/pdf/2307.08015.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Boosting3DoFAccuracy"> code </a>
              <p></p>
              <p> This work proposes a decoupled rotation and translation estimation method for cross-view image matching, achieving significant performance improvement. 
              </p> -->
	      <p></p>
            </td>
        </tr>


	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/LetsGo.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2502.03498">
                <papertitle>Letsgo: Large-scale garage modeling and rendering via lidar-assisted gaussian primitives </papertitle>
              </a>
              <br>
	      Jiadi Cui, Junming Cao, Fuqiang Zhao, Zhipeng He, Yifan Chen, Yuhui Zhong, Lan Xu, <strong>Yujiao Shi</strong> *, Yingliang Zhang *, Jingyi Yu *
              <br>
              ACM Transactions on Graphics (TOG), 2024
              <br>
<!--               <a href="https://arxiv.org/pdf/2307.08015.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Boosting3DoFAccuracy"> code </a>
              <p></p>
              <p> This work proposes a decoupled rotation and translation estimation method for cross-view image matching, achieving significant performance improvement. 
              </p> -->
	      <p></p>
            </td>
		
        </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/FastGrasp.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a>
                <papertitle>FastGrasp: Efficient Grasp Synthesis with Diffusion </papertitle>
              </a>
              <br>
	       Xiaofei Wu, Tao Liu, Caoji Li, Yuexin Ma, <strong>Yujiao Shi </strong>*, and Xuming He* 
              <br>
              3DV, 2025
              <br>
<!--               <a href="https://arxiv.org/pdf/2307.08015.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Boosting3DoFAccuracy"> code </a>
              <p></p>
              <p> This work proposes a decoupled rotation and translation estimation method for cross-view image matching, achieving significant performance improvement. 
              </p> -->
	      <p></p>
            </td>
        </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/3DV25.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a>
                <papertitle>Geometry-guided Cross-view Diffusion for One-to-many Cross-view Image Synthesis </papertitle>
              </a>
              <br>
	       Tao Jun Lin, Wenqing Wang, <strong>Yujiao Shi </strong>, Akhil Perincherry, Ankit Vora, and Hongdong Li
              <br>
              3DV, 2025
              <br>
<!--               <a href="https://arxiv.org/pdf/2307.08015.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Boosting3DoFAccuracy"> code </a>
              <p></p>
              <p> This work proposes a decoupled rotation and translation estimation method for cross-view image matching, achieving significant performance improvement. 
              </p> -->
	      <p></p>
            </td>
        </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/G2SWeakly.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a>
                <papertitle>Weakly-supervised camera localization by ground-to-satellite image registration </papertitle>
              </a>
              <br>
	       <strong>Yujiao Shi </strong>, Akhil Perincherry, Ankit Vora, and Hongdong Li
              <br>
              ECCV, 2024
              <br>
<!--               <a href="https://arxiv.org/pdf/2307.08015.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Boosting3DoFAccuracy"> code </a>
              <p></p>
              <p> This work proposes a decoupled rotation and translation estimation method for cross-view image matching, achieving significant performance improvement. 
              </p> -->
	      <p></p>
            </td>
        </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Adapting.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2502.03498">
                <papertitle>Adapting fine-grained cross-view localization to areas without fine ground truth </papertitle>
              </a>
              <br>
	      Zimin Xia, <strong>Yujiao Shi </strong>, Hongdong Li, and Julian FP Kooij
              <br>
              ECCV, 2024
              <br>
<!--               <a href="https://arxiv.org/pdf/2307.08015.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Boosting3DoFAccuracy"> code </a>
              <p></p>
              <p> This work proposes a decoupled rotation and translation estimation method for cross-view image matching, achieving significant performance improvement. 
              </p> -->
            </td>
        </tr>
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Boosting" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Boosting_3-DoF_Ground-to-Satellite_Camera_Localization_Accuracy_via_Geometry-Guided_Cross-View_Transformer_ICCV_2023_paper.pdf">
                <papertitle>Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via
Geometry-Guided Cross-View Transformer </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Fei Wu,  Akhil Perincherry, Ankit Vora, and Hongdong Li
              <br>
              ICCV, 2023
              <br>
              <a href="https://arxiv.org/pdf/2307.08015.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Boosting3DoFAccuracy"> code </a>
              <p></p>
              <p> This work proposes a decoupled rotation and translation estimation method for cross-view image matching, achieving significant performance improvement. 
              </p>
            </td>
        </tr>
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVLNet" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Shi_CVLNet_Cross-View_Feature_Correspondence_Learning_for_Video-based_Camera_Localization_ACCV_2022_paper.pdf">
                <papertitle>CVLNet: Cross-View Semantic Correspondence Learning for Video-based Camera Localization </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Shan Wang, and Hongdong Li
              <br>
              ACCV, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Shi_CVLNet_Cross-View_Feature_Correspondence_Learning_for_Video-based_Camera_Localization_ACCV_2022_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/CVLNet"> code </a>
              <p></p>
              <p> This work addresses city-scale satellite image-based camera localization by using a sequence of ground-view images.
              </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/HighlyAccurate.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_Beyond_Cross-View_Image_Retrieval_Highly_Accurate_Vehicle_Localization_Using_Satellite_CVPR_2022_paper.pdf">
                <papertitle>Beyond Cross-view Image Retrieval: Highly Accurate Vehicle Localization Using Satellite Image </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Hongdong Li
              <br>
              CVPR, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_Beyond_Cross-View_Image_Retrieval_Highly_Accurate_Vehicle_Localization_Using_Satellite_CVPR_2022_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/HighlyAccurate"> code </a>
              <p></p>
              <p> We introduce a new pose optimization method to accurately pinpoint which pixel in a satellite image corresponds to the query camera location.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/3DOF.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2203.14148.pdf">
                <papertitle>Accurate 3-DoF Camera Geo-Localization via Ground-to-Satellite Image Matching </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Liu Liu, Dylan Campbell, Piotr Koniusz, and Hongdong Li
              <br>
              TPAMI, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.14148.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/IBL"> code </a>
              <p></p>
              <p> We propose projective transform, which (1) compliments polar transform to achieve better coarse localization performance and (2) provides a novel handcrafted method to accurately localize query camera on its matching satellite image.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Sat2Str.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2103.01623.pdf">
                <papertitle>Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Dylan Campbell, Xin Yu, and Hongdong Li
              <br>
              TPAMI, 2022
              <br>
              <a href="https://arxiv.org/pdf/2103.01623.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Sat2StrPanoramaSynthesis"> code </a>
              <p></p>
              <p> Satellite to street-view panorama synthesis, implicit satellite image height map estimation.
              </p>
            </td>
          </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SVNVS.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Self-Supervised_Visibility_Learning _for_Novel_View_Synthesis_CVPR_2021_paper.pdf">
                <papertitle>Self-Supervised Visibility Learning for Novel View Synthesis </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Hongdong Li, and Xin Yu
              <br>
              CVPR, 2021
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Self-Supervised_Visibility_Learning_for_Novel_View_Synthesis_CVPR_2021_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/SVNVS"> code </a>
              <p></p>
              <p> We estimate target-view depth and source-view visibility in an end-to-end manner.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/DSM.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Where_Am_I_Looking_At_Joint_Location_and_Orientation_Estimation_CVPR_2020_paper.pdf">
                <papertitle>Where am I looking at? Joint Location and Orientation Estimation by Cross-View Matching </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Dylan Campbell, and Hongdong Li
              <br>
              CVPR, 2020
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Where_Am_I_Looking_At_Joint_Location_and_Orientation_Estimation_CVPR_2020_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_DSM"> code </a>
              <p></p>
              <p> The first 3-DoF camera pose estimation framework via ground-to-satellite image matching.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVFT.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6875">
                <papertitle>Optimal Feature Transport for Cross-View Image Geo-Localization </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Liu Liu, Tong Zhang, and Hongdong Li
              <br>
              AAAI, 2020
              <br>
              <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6875"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_CVFT"> code </a>
              <p></p>
              <p> Motivated by optimal transport, we invent a cross-view feature transport module to bridge the cross-view domain gap.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SAFA.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf">
                <papertitle>Spatial-Aware Feature Aggregation for Cross-View Image based Geo-Localization </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Liu Liu, Xin Yu, and Hongdong Li
              <br>
              NeurIPS, 2019
              <br>
              <a href="https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_SAFA"> code </a>
              <p></p>
              <p> A polar transform is introduced to bridge the ground-and-satellite domain gap, which significantly boosts the state-of-the-art cross-view localization performance.
              </p>
            </td>
          </tr>


        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Service</heading>
              <p> Conference Program Committee Member/Reviewer:
              </p>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Conference on Computer Vision (ICCV)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">European Conference on Computer Vision (ECCV)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Asian Conference on Computer Vision (ACCV)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Neural Information Processing Systems (NeurIPS)</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The International Conference on Learning Representations (ICLR)</li> 
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">AAAI Conference on Artificial Intelligence (AAAI)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Joint Conference on Artificial Intelligence (IJCAI)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Conference on 3D Vision (3DV)</li>

                </ul>
              </p>
              <p> Journal Reviewer:
              </p>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Journal of Computer Vision (IJCV)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Image Processing (TIP)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The IEEE Robotics and Automation Letters (RAL)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Artificial Intelligence</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Geoscience and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ISPRS Journal of Photogrammetry and Remote Sensing</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Journal of Digital Earth</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Knowledge-Based Systems</li>
                    
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        

					
        </tbody></table>
        <p align="center">
          <font size="2">
            Template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
          </font>
        </p>
      </td>
    </tr>
  </table>
</body>

</html>
